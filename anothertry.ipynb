{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\agent\\AppData\\Roaming\\Python\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "from moviepy.editor import VideoFileClip, AudioFileClip, concatenate_audioclips\n",
    "from torch.nn.functional import pad\n",
    "from pydub import AudioSegment\n",
    "import numpy as np\n",
    "from gtts import gTTS\n",
    "\n",
    "import time\n",
    "import tempfile\n",
    "import os\n",
    "import pyttsx3\n",
    "import torch\n",
    "import torchaudio\n",
    "import gc\n",
    "import subprocess\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-large\")\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-large\")\n",
    "translator_model_name = \"Helsinki-NLP/opus-mt-fr-en\"\n",
    "tokenizer = MarianTokenizer.from_pretrained(translator_model_name)\n",
    "translator = MarianMTModel.from_pretrained(translator_model_name)\n",
    "\n",
    "def transcribe_audio(audio_path, target_lang=\"en\"):\n",
    "    waveform, rate = torchaudio.load(audio_path)\n",
    "    if waveform.dim() > 2:\n",
    "        waveform = waveform.mean(dim=0, keepdim=True)\n",
    "    if rate != 16000:\n",
    "        resampler = torchaudio.transforms.Resample(orig_freq=rate, new_freq=16000)\n",
    "        waveform = resampler(waveform)\n",
    "    waveform = waveform.squeeze(0)\n",
    "    inputs = processor(waveform.numpy(), sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
    "    input_features = inputs.input_features\n",
    "    attention_mask = inputs.attention_mask if 'attention_mask' in inputs else None\n",
    "\n",
    "    with torch.no_grad():\n",
    "        predicted_ids = model.generate(input_features, language=target_lang, attention_mask=attention_mask)\n",
    "    transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n",
    "    print(transcription)\n",
    "    return transcription\n",
    "\n",
    "def translate_text(text, target_lang=\"en\"):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True)\n",
    "    translated = translator.generate(**inputs)\n",
    "    translated_text = tokenizer.decode(translated[0], skip_special_tokens=True)\n",
    "    print(f'Original text : {inputs.input_ids}')\n",
    "    print(f'Translated text : {translated_text}')\n",
    "    return translated_text\n",
    "\n",
    "def text_to_speech(text, output_audio_path=\"translated_audio.mp3\", language_code=\"en-US\"):\n",
    "    engine = pyttsx3.init()\n",
    "    engine.setProperty(\"rate\", 150)  # Adjust speed as needed for sync\n",
    "    if language_code == \"en-US\":\n",
    "        engine.setProperty(\"voice\", \"com.apple.speech.synthesis.voice.Alex\")\n",
    "    else:\n",
    "        engine.setProperty(\"voice\", \"com.apple.speech.synthesis.voice.Thomas\")\n",
    "    engine.save_to_file(text, output_audio_path)\n",
    "    engine.runAndWait()\n",
    "    return output_audio_path\n",
    "\n",
    "def convert_mp3_to_wav(mp3_path, wav_path=\"converted_audio.wav\"):\n",
    "    audio = AudioSegment.from_mp3(mp3_path).set_channels(1)  # Convert to mono\n",
    "    audio.export(wav_path, format=\"wav\")\n",
    "    return wav_path\n",
    "\n",
    "def add_audio_to_video(video_path, audio_path, output_path=\"output_video.mp4\"):\n",
    "    import time\n",
    "    from moviepy.editor import VideoFileClip, AudioFileClip\n",
    "\n",
    "    video_clip = VideoFileClip(video_path)\n",
    "    audio_clip = AudioFileClip(audio_path)\n",
    "    try:\n",
    "        # Synchronize audio duration with video duration\n",
    "        audio_clip = audio_clip.set_duration(video_clip.duration)\n",
    "        video_clip = video_clip.set_audio(audio_clip)\n",
    "        video_clip.write_videofile(output_path, codec=\"libx264\", audio_codec=\"aac\", remove_temp=True)\n",
    "    finally:\n",
    "        # Close clips to ensure resources are released\n",
    "        audio_clip.close()\n",
    "        video_clip.close()\n",
    "        del audio_clip, video_clip\n",
    "        gc.collect()\n",
    "        time.sleep(1)  # Optional: Introduce a delay to ensure complete cleanup\n",
    "\n",
    "\n",
    "\n",
    "def process_input(input_path, input_type, target_lang=\"en\"):\n",
    "    from pydub import AudioSegment\n",
    "    from moviepy.editor import concatenate_audioclips, AudioFileClip\n",
    "\n",
    "    if input_type == \"video\":\n",
    "        video_clip = VideoFileClip(input_path)\n",
    "        audio_path = \"extracted_audio.wav\"\n",
    "        video_clip.audio.write_audiofile(audio_path)\n",
    "    elif input_type == \"audio\":\n",
    "        audio_path = input_path\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported input type. Please use 'video' or 'audio'.\")\n",
    "\n",
    "    # Split audio into segments for synchronization with precise timestamps\n",
    "    segment_duration = 1  # seconds (adjust as needed)\n",
    "    audio = AudioSegment.from_file(audio_path)\n",
    "    num_segments = len(audio) // (1000 * segment_duration) + 1\n",
    "    audio_chunks = [audio[i * 1000 * segment_duration:(i + 1) * 1000 * segment_duration]\n",
    "                    for i in range(num_segments)]\n",
    "\n",
    "    translated_audio_segments = []\n",
    "    for i, chunk in enumerate(audio_chunks):\n",
    "        chunk_path = f\"chunk_{i}.wav\"\n",
    "        chunk.export(chunk_path, format=\"wav\")\n",
    "        original_text = transcribe_audio(chunk_path)\n",
    "        translated_text = translate_text(original_text, target_lang)\n",
    "        translated_chunk_path = f\"translated_chunk_{i}.mp3\"\n",
    "        text_to_speech(translated_text, output_audio_path=translated_chunk_path)\n",
    "        translated_audio_segments.append(AudioFileClip(translated_chunk_path))\n",
    "\n",
    "    # Concatenate all translated audio segments with precise synchronization\n",
    "    final_audio = concatenate_audioclips(translated_audio_segments)\n",
    "    final_audio_path = \"final_translated_audio.mp3\"\n",
    "    final_audio.write_audiofile(final_audio_path)\n",
    "\n",
    "    # Add the final synchronized audio to the video\n",
    "    if input_type == \"video\":\n",
    "        output_path = \"translated_video.mp4\"\n",
    "        add_audio_to_video(input_path, final_audio_path, output_path=output_path)\n",
    "    else:\n",
    "        output_path = final_audio_path\n",
    "\n",
    "    return output_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-large\")\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-large\")\n",
    "translator_model_name = \"Helsinki-NLP/opus-mt-fr-en\"\n",
    "tokenizer = MarianTokenizer.from_pretrained(translator_model_name)\n",
    "translator = MarianMTModel.from_pretrained(translator_model_name)\n",
    "\n",
    "def pad_mel_features(mel_features, target_length=3000):\n",
    "    # Pad mel features to match the expected length of 3000\n",
    "    print(f'Length of mel_features : {mel_features.shape}')\n",
    "    current_length = mel_features.shape[-1]\n",
    "    if current_length < target_length:\n",
    "        mel_features = pad(mel_features, (0, target_length - current_length), mode='constant', value=0)\n",
    "    print(f'Length of mel_features : {mel_features.shape}')\n",
    "    return mel_features\n",
    "\n",
    "def transcribe_audio(audio_path):\n",
    "    waveform, rate = torchaudio.load(audio_path)\n",
    "    print(f'Rate : {rate}')\n",
    "    print(f'Dims of waveform : {waveform.shape}')\n",
    "    if waveform.dim() >= 2:\n",
    "        waveform = waveform.mean(dim=0, keepdim=True)\n",
    "    print(f'Dims of waveform : {waveform.shape}')\n",
    "    if rate != 16000:\n",
    "        resampler = torchaudio.transforms.Resample(orig_freq=rate, new_freq=16000)\n",
    "        waveform = resampler(waveform)\n",
    "    waveform = waveform.squeeze(0)\n",
    "    print(f'Dims of waveform : {waveform.shape}')\n",
    "    inputs = processor(waveform.numpy(), sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
    "    input_features = inputs.input_features\n",
    "\n",
    "    # Ensure input features are correctly padded to 3000\n",
    "    input_features = pad_mel_features(input_features)\n",
    "    print(f'Data type of inputs : {inputs.keys()}')\n",
    "    print(f'Number of keys of inputs : {len(inputs)}')\n",
    "    print(f'Shape of input_features : {input_features.shape}')\n",
    "\n",
    "    # Generate transcription\n",
    "    with torch.no_grad():\n",
    "        predicted_ids = model.generate(input_features)\n",
    "        print(f'Shape of predicted_ids : {predicted_ids.shape}')\n",
    "    transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n",
    "    print(transcription)\n",
    "    return transcription\n",
    "\n",
    "def translate_text(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True)\n",
    "    translated = translator.generate(**inputs)\n",
    "    translated_text = tokenizer.decode(translated[0], skip_special_tokens=True)\n",
    "    print(f\"Translated text: {translated_text}\")\n",
    "    return translated_text\n",
    "\n",
    "def text_to_speech(text, output_audio_path=\"translated_audio.mp3\", language_code=\"en-US\"):\n",
    "    engine = pyttsx3.init()\n",
    "    engine.setProperty(\"rate\", 150)  # Adjust speed for synchronization\n",
    "    if language_code == \"en-US\":\n",
    "        engine.setProperty(\"voice\", \"com.apple.speech.synthesis.voice.Alex\")\n",
    "    else:\n",
    "        engine.setProperty(\"voice\", \"com.apple.speech.synthesis.voice.Thomas\")\n",
    "    engine.save_to_file(text, output_audio_path)\n",
    "    engine.runAndWait()\n",
    "    return output_audio_path\n",
    "def add_audio_to_video(video_path, audio_path, output_path=\"output_video.mp4\"):\n",
    "    # Load the original video\n",
    "    video = VideoFileClip(video_path)\n",
    "    \n",
    "    # Load the new audio\n",
    "    new_audio = AudioFileClip(audio_path)\n",
    "    \n",
    "    # Set the new audio to the video\n",
    "    video_with_new_audio = video.set_audio(new_audio)\n",
    "    \n",
    "    # Write the result to a file\n",
    "    video_with_new_audio.write_videofile(output_path, codec=\"libx264\", audio_codec=\"aac\")\n",
    "\n",
    "def process_input(file_path, input_type, target_lang=\"en\"):\n",
    "    try:\n",
    "        # Step 1: Extract audio from video if the input is a video\n",
    "        if input_type == \"video\":\n",
    "            audio_path = extract_audio(file_path)\n",
    "            if audio_path is None:\n",
    "                print(\"Error: Failed to extract audio from video.\")\n",
    "                return None  # Exit if audio extraction fails\n",
    "        else:\n",
    "            audio_path = file_path  # Assume it's already an audio file\n",
    "\n",
    "        # Step 2: Transcribe the audio\n",
    "        original_transcription = transcribe_audio(audio_path)\n",
    "        print(f'DataType of transcription {type(original_transcription)}')\n",
    "\n",
    "        # Step 3: Translate the transcription to the target language\n",
    "        translated_text = translate_text(original_transcription)\n",
    "\n",
    "        # Step 4: Align the translated audio with the original timing\n",
    "        aligned_audio_path = process_audio_and_align(audio_path, translated_text)\n",
    "        if aligned_audio_path is None:\n",
    "            print(\"Error: Failed to process and align audio.\")\n",
    "            return None  # Exit if audio processing fails\n",
    "\n",
    "        # Step 5: Add the aligned audio to the video (if input is a video)\n",
    "        if input_type == \"video\":\n",
    "            output_path = \"translated_video.mp4\"\n",
    "            add_audio_to_video(file_path, aligned_audio_path, output_path)\n",
    "        else:\n",
    "            output_path = aligned_audio_path\n",
    "\n",
    "        return output_path\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during the process: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "def extract_audio(video_path, output_audio_path=\"extracted_audio.wav\"):\n",
    "    try:\n",
    "        video = VideoFileClip(video_path)\n",
    "        audio = video.audio\n",
    "        audio.write_audiofile(output_audio_path)\n",
    "        return output_audio_path\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting audio: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "import speech_recognition as sr\n",
    "\n",
    "def get_word_timings(original_audio, recognized_text):\n",
    "    \"\"\"\n",
    "    Get the approximate start and end times for each word in the original audio.\n",
    "    \"\"\"\n",
    "    recognizer = sr.Recognizer()\n",
    "    with sr.AudioFile(original_audio) as source:\n",
    "        audio = recognizer.record(source)\n",
    "    try:\n",
    "        # Perform speech-to-text using Google Web Speech API\n",
    "        transcription = recognizer.recognize_google(audio)\n",
    "        print(f\"Recognized Text: {transcription}\")\n",
    "\n",
    "        # Split recognized text and the translated text into words\n",
    "        original_words = transcription.split()\n",
    "        translated_words = recognized_text.split()\n",
    "\n",
    "        # Calculate the word timings based on the word length in both texts\n",
    "        word_durations = len(original_audio) / len(original_words)  # approximate duration per word\n",
    "        timings = []\n",
    "        current_time = 0\n",
    "\n",
    "        for i, word in enumerate(original_words):\n",
    "            # For each word, calculate its start and end time\n",
    "            end_time = current_time + word_durations\n",
    "            timings.append((current_time, end_time))\n",
    "            current_time = end_time\n",
    "\n",
    "        return timings\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error recognizing speech: {e}\")\n",
    "        return []\n",
    "\n",
    "def process_audio_and_align(audio_path, translated_text):\n",
    "    original_audio = AudioSegment.from_file(audio_path)\n",
    "    audio_duration = len(original_audio) / 1000  # in seconds\n",
    "    print(f'Audio duration: {audio_duration}')\n",
    "    \n",
    "    words = translated_text.split()\n",
    "    print(f'Translated words: {words}')\n",
    "\n",
    "    # Get the word timings from the original audio using speech recognition\n",
    "    word_timings = get_word_timings(audio_path, translated_text)\n",
    "\n",
    "    if not word_timings:\n",
    "        print(\"Unable to get word timings, skipping alignment.\")\n",
    "        return None\n",
    "\n",
    "    # Initialize the text-to-speech engine\n",
    "    engine = pyttsx3.init()\n",
    "    engine.setProperty(\"rate\", 100)\n",
    "    engine.setProperty(\"voice\", \"com.apple.speech.synthesis.voice.Alex\")\n",
    "\n",
    "    # Generate the entire sentence audio in one go (instead of generating per word)\n",
    "    sentence_audio_buffer = io.BytesIO()\n",
    "    engine.save_to_file(translated_text, sentence_audio_buffer)\n",
    "    engine.runAndWait()\n",
    "    \n",
    "    # Load the generated audio into AudioSegment from memory\n",
    "    sentence_audio_buffer.seek(0)  # Rewind the buffer to start\n",
    "    full_audio = AudioSegment.from_file(sentence_audio_buffer, format=\"wav\")\n",
    "    print(f\"Full sentence audio generated and loaded.\")\n",
    "\n",
    "    # Split the audio into word-level segments based on the recognized timings\n",
    "    speech_segments = []\n",
    "    for i, (start_time, end_time) in enumerate(word_timings):\n",
    "        # Extract the corresponding word audio segment from the full audio\n",
    "        word_audio = full_audio[start_time * 1000:end_time * 1000]  # Convert to milliseconds\n",
    "        speech_segments.append(word_audio)\n",
    "        \n",
    "        # Add pause if needed based on the original audio\n",
    "        if i < len(word_timings) - 1:\n",
    "            next_start_time = word_timings[i + 1][0]\n",
    "            pause_duration = (next_start_time - end_time) * 1000  # pause duration in milliseconds\n",
    "            if pause_duration > 0:\n",
    "                speech_segments.append(AudioSegment.silent(duration=pause_duration))\n",
    "\n",
    "    try:\n",
    "        # Combine all word audio segments into one aligned audio\n",
    "        aligned_audio = sum(speech_segments) if all(isinstance(seg, AudioSegment) for seg in speech_segments) else None\n",
    "        if aligned_audio is None:\n",
    "            raise ValueError(\"Invalid segments in speech segments.\")\n",
    "        \n",
    "        # Export aligned audio to a file if needed, for example:\n",
    "        aligned_audio.export(\"aligned_audio.wav\", format=\"wav\")\n",
    "        print(\"Aligned audio exported successfully.\")\n",
    "        return \"aligned_audio.wav\"  # Return the file path of the final output\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error exporting aligned audio: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in final_translated_audio.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# Use the converted WAV file in the pipeline\n",
    "audio_path = convert_mp3_to_wav(\"1108.MP3\")\n",
    "translated_audio = process_input(audio_path, \"audio\", target_lang=\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in extracted_audio.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Rate : 44100\n",
      "Dims of waveform : torch.Size([2, 228438])\n",
      "Dims of waveform : torch.Size([1, 228438])\n",
      "Dims of waveform : torch.Size([82880])\n",
      "Length of mel_features : torch.Size([1, 80, 518])\n",
      "Length of mel_features : torch.Size([1, 80, 3000])\n",
      "Data type of inputs : dict_keys(['input_features'])\n",
      "Number of keys of inputs : 1\n",
      "Shape of input_features : torch.Size([1, 80, 3000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of predicted_ids : torch.Size([1, 9])\n",
      " Je vis en France.\n",
      "DataType of transcription <class 'str'>\n",
      "Translated text: I live in France.\n",
      "Audio duration: 5.18\n",
      "Translated words: ['I', 'live', 'in', 'France.']\n",
      "Recognized Text: Jab beta France\n",
      "An error occurred during the process: Decoding failed. ffmpeg returned error code: 3199971767\n",
      "\n",
      "Output from ffmpeg/avlib:\n",
      "\n",
      "ffmpeg version 2024-11-03-git-df00705e00-full_build-www.gyan.dev Copyright (c) 2000-2024 the FFmpeg developers\n",
      "  built with gcc 14.2.0 (Rev1, Built by MSYS2 project)\n",
      "  configuration: --enable-gpl --enable-version3 --enable-static --disable-w32threads --disable-autodetect --enable-fontconfig --enable-iconv --enable-gnutls --enable-libxml2 --enable-gmp --enable-bzlib --enable-lzma --enable-libsnappy --enable-zlib --enable-librist --enable-libsrt --enable-libssh --enable-libzmq --enable-avisynth --enable-libbluray --enable-libcaca --enable-sdl2 --enable-libaribb24 --enable-libaribcaption --enable-libdav1d --enable-libdavs2 --enable-libopenjpeg --enable-libquirc --enable-libuavs3d --enable-libxevd --enable-libzvbi --enable-libqrencode --enable-librav1e --enable-libsvtav1 --enable-libvvenc --enable-libwebp --enable-libx264 --enable-libx265 --enable-libxavs2 --enable-libxeve --enable-libxvid --enable-libaom --enable-libjxl --enable-libvpx --enable-mediafoundation --enable-libass --enable-frei0r --enable-libfreetype --enable-libfribidi --enable-libharfbuzz --enable-liblensfun --enable-libvidstab --enable-libvmaf --enable-libzimg --enable-amf --enable-cuda-llvm --enable-cuvid --enable-dxva2 --enable-d3d11va --enable-d3d12va --enable-ffnvcodec --enable-libvpl --enable-nvdec --enable-nvenc --enable-vaapi --enable-libshaderc --enable-vulkan --enable-libplacebo --enable-opencl --enable-libcdio --enable-libgme --enable-libmodplug --enable-libopenmpt --enable-libopencore-amrwb --enable-libmp3lame --enable-libshine --enable-libtheora --enable-libtwolame --enable-libvo-amrwbenc --enable-libcodec2 --enable-libilbc --enable-libgsm --enable-liblc3 --enable-libopencore-amrnb --enable-libopus --enable-libspeex --enable-libvorbis --enable-ladspa --enable-libbs2b --enable-libflite --enable-libmysofa --enable-librubberband --enable-libsoxr --enable-chromaprint\n",
      "  libavutil      59. 46.100 / 59. 46.100\n",
      "  libavcodec     61. 24.100 / 61. 24.100\n",
      "  libavformat    61.  9.100 / 61.  9.100\n",
      "  libavdevice    61.  4.100 / 61.  4.100\n",
      "  libavfilter    10.  6.101 / 10.  6.101\n",
      "  libswscale      8.  9.101 /  8.  9.101\n",
      "  libswresample   5.  4.100 /  5.  4.100\n",
      "  libpostproc    58.  4.100 / 58.  4.100\n",
      "[wav @ 00000233b5bfffc0] invalid start code [0][0][0][0] in RIFF header\n",
      "[cache @ 00000233b5c00500] Statistics, cache hits:0 cache misses:0\n",
      "[in#0 @ 00000233b5bffc00] Error opening input: Invalid data found when processing input\n",
      "Error opening input file cache:pipe:0.\n",
      "Error opening input files: Invalid data found when processing input\n",
      "\n"
     ]
    }
   ],
   "source": [
    "translated_audio = process_input(\"Recording 2024-11-08 233058.mp4\", \"video\", target_lang=\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
