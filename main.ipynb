{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'moviepy.editor'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WhisperProcessor, WhisperForConditionalGeneration\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MarianMTModel, MarianTokenizer\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmoviepy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meditor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VideoFileClip, AudioFileClip, concatenate_audioclips\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydub\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AudioSegment\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'moviepy.editor'"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "from moviepy.editor import VideoFileClip, AudioFileClip, concatenate_audioclips\n",
    "from pydub import AudioSegment\n",
    "import numpy as np\n",
    "\n",
    "import pyttsx3\n",
    "import torch\n",
    "import torchaudio\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-large\")\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-large\")\n",
    "translator_model_name = \"Helsinki-NLP/opus-mt-fr-en\"\n",
    "tokenizer = MarianTokenizer.from_pretrained(translator_model_name)\n",
    "translator = MarianMTModel.from_pretrained(translator_model_name)\n",
    "\n",
    "def transcribe_audio(audio_path, target_lang=\"en\"):\n",
    "    waveform, rate = torchaudio.load(audio_path)\n",
    "    if waveform.dim() > 2:\n",
    "        waveform = waveform.mean(dim=0, keepdim=True)\n",
    "    if rate != 16000:\n",
    "        resampler = torchaudio.transforms.Resample(orig_freq=rate, new_freq=16000)\n",
    "        waveform = resampler(waveform)\n",
    "    waveform = waveform.squeeze(0)\n",
    "    inputs = processor(waveform.numpy(), sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
    "    input_features = inputs.input_features\n",
    "    attention_mask = inputs.attention_mask if 'attention_mask' in inputs else None\n",
    "\n",
    "    with torch.no_grad():\n",
    "        predicted_ids = model.generate(\n",
    "            input_features, language=target_lang, attention_mask=attention_mask\n",
    "        )\n",
    "    transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n",
    "    return transcription\n",
    "\n",
    "def translate_text(text, target_lang=\"en\"):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True)\n",
    "    translated = translator.generate(**inputs)\n",
    "    translated_text = tokenizer.decode(translated[0], skip_special_tokens=True)\n",
    "    return translated_text\n",
    "\n",
    "def text_to_speech(text, output_audio_path=\"translated_audio.mp3\", language_code=\"en-US\"):\n",
    "    engine = pyttsx3.init()\n",
    "    engine.setProperty(\"rate\", 150)  # Adjust speed as needed for sync\n",
    "    if language_code == \"en-US\":\n",
    "        engine.setProperty(\"voice\", \"com.apple.speech.synthesis.voice.Alex\")\n",
    "    else:\n",
    "        engine.setProperty(\"voice\", \"com.apple.speech.synthesis.voice.Thomas\")\n",
    "    engine.save_to_file(text, output_audio_path)\n",
    "    engine.runAndWait()\n",
    "    return output_audio_path\n",
    "\n",
    "def convert_mp3_to_wav(mp3_path, wav_path=\"converted_audio.wav\"):\n",
    "    audio = AudioSegment.from_mp3(mp3_path).set_channels(1)  # Convert to mono\n",
    "    audio.export(wav_path, format=\"wav\")\n",
    "    return wav_path\n",
    "\n",
    "def add_audio_to_video(video_path, audio_path, output_path=\"output_video.mp4\"):\n",
    "    import time\n",
    "    from moviepy.editor import VideoFileClip, AudioFileClip\n",
    "\n",
    "    video_clip = VideoFileClip(video_path)\n",
    "    audio_clip = AudioFileClip(audio_path)\n",
    "    try:\n",
    "        # Synchronize audio duration with video duration\n",
    "        audio_clip = audio_clip.set_duration(video_clip.duration)\n",
    "        video_clip = video_clip.set_audio(audio_clip)\n",
    "        video_clip.write_videofile(output_path, codec=\"libx264\", audio_codec=\"aac\", remove_temp=True)\n",
    "    finally:\n",
    "        # Close clips to ensure resources are released\n",
    "        audio_clip.close()\n",
    "        video_clip.close()\n",
    "        del audio_clip, video_clip\n",
    "        gc.collect()\n",
    "        time.sleep(1)  # Optional: Introduce a delay to ensure complete cleanup\n",
    "\n",
    "\n",
    "\n",
    "def process_input(input_path, input_type, target_lang=\"en\"):\n",
    "    if input_type == \"video\":\n",
    "        video_clip = VideoFileClip(input_path)\n",
    "        audio_path = \"extracted_audio.wav\"\n",
    "        video_clip.audio.write_audiofile(audio_path)\n",
    "    elif input_type == \"audio\":\n",
    "        audio_path = input_path\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported input type. Please use 'video' or 'audio'.\")\n",
    "\n",
    "    # Split audio into segments for synchronization\n",
    "    segment_duration = 1  # seconds (adjust as needed)\n",
    "    audio = AudioSegment.from_file(audio_path)\n",
    "    audio_chunks = [audio[i * 1000 * segment_duration:(i + 1) * 1000 * segment_duration]\n",
    "                    for i in range(len(audio) // (1000 * segment_duration))]\n",
    "\n",
    "    translated_audio_segments = []\n",
    "    for i, chunk in enumerate(audio_chunks):\n",
    "        chunk_path = f\"chunk_{i}.wav\"\n",
    "        chunk.export(chunk_path, format=\"wav\")\n",
    "        original_text = transcribe_audio(chunk_path)\n",
    "        translated_text = translate_text(original_text, target_lang)\n",
    "        translated_chunk_path = f\"translated_chunk_{i}.mp3\"\n",
    "        text_to_speech(translated_text, output_audio_path=translated_chunk_path)\n",
    "        translated_audio_segments.append(AudioFileClip(translated_chunk_path))\n",
    "\n",
    "    # Concatenate all translated audio segments\n",
    "    final_audio = concatenate_audioclips(translated_audio_segments)\n",
    "    final_audio_path = \"final_translated_audio.mp3\"\n",
    "    final_audio.write_audiofile(final_audio_path)\n",
    "\n",
    "    # Add the final synchronized audio to the video\n",
    "    if input_type == \"video\":\n",
    "        output_path = \"translated_video.mp4\"\n",
    "        add_audio_to_video(input_path, final_audio_path, output_path=output_path)\n",
    "    else:\n",
    "        output_path = final_audio_path\n",
    "\n",
    "    return output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in extracted_audio.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You have passed language=en, but also have set `forced_decoder_ids` to [[1, None], [2, 50359]] which creates a conflict. `forced_decoder_ids` will be ignored in favor of language=en.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in final_translated_audio.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Building video translated_video.mp4.\n",
      "MoviePy - Writing audio in translated_videoTEMP_MPY_wvf_snd.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video translated_video.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready translated_video.mp4\n"
     ]
    }
   ],
   "source": [
    "translated_audio = process_input(\"Recording 2024-11-08 233058.mp4\", \"video\", target_lang=\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Couldn't find appropriate backend to handle uri Manav Kaul's lay 'Bali aur Shambhu' by Leher, The Dramatics Society of DCAC, Delhi University. [MZlTjI4sa68].mp3 and format None.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 35\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m transcription\n\u001b[0;32m     34\u001b[0m audio_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mManav Kaul\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms lay \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBali aur Shambhu\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m by Leher, The Dramatics Society of DCAC, Delhi University. [MZlTjI4sa68].mp3\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTranscription:\u001b[39m\u001b[38;5;124m\"\u001b[39m, transcribe_hindi_audio(audio_path))\n",
      "Cell \u001b[1;32mIn[3], line 11\u001b[0m, in \u001b[0;36mtranscribe_hindi_audio\u001b[1;34m(audio_path)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtranscribe_hindi_audio\u001b[39m(audio_path):\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m# Load and preprocess audio\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m     waveform, rate \u001b[38;5;241m=\u001b[39m torchaudio\u001b[38;5;241m.\u001b[39mload(audio_path)\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m waveform\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m     13\u001b[0m         waveform \u001b[38;5;241m=\u001b[39m waveform\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\agent\\anaconda3\\Lib\\site-packages\\torchaudio\\_backend\\utils.py:204\u001b[0m, in \u001b[0;36mget_load_func.<locals>.load\u001b[1;34m(uri, frame_offset, num_frames, normalize, channels_first, format, buffer_size, backend)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\n\u001b[0;32m    119\u001b[0m     uri: Union[BinaryIO, \u001b[38;5;28mstr\u001b[39m, os\u001b[38;5;241m.\u001b[39mPathLike],\n\u001b[0;32m    120\u001b[0m     frame_offset: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    126\u001b[0m     backend: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    127\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;28mint\u001b[39m]:\n\u001b[0;32m    128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load audio data from source.\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \n\u001b[0;32m    130\u001b[0m \u001b[38;5;124;03m    By default (``normalize=True``, ``channels_first=True``), this function returns Tensor with\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;124;03m            `[channel, time]` else `[time, channel]`.\u001b[39;00m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 204\u001b[0m     backend \u001b[38;5;241m=\u001b[39m dispatcher(uri, \u001b[38;5;28mformat\u001b[39m, backend)\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mload(uri, frame_offset, num_frames, normalize, channels_first, \u001b[38;5;28mformat\u001b[39m, buffer_size)\n",
      "File \u001b[1;32mc:\\Users\\agent\\anaconda3\\Lib\\site-packages\\torchaudio\\_backend\\utils.py:116\u001b[0m, in \u001b[0;36mget_load_func.<locals>.dispatcher\u001b[1;34m(uri, format, backend_name)\u001b[0m\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mcan_decode(uri, \u001b[38;5;28mformat\u001b[39m):\n\u001b[0;32m    115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m backend\n\u001b[1;32m--> 116\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find appropriate backend to handle uri \u001b[39m\u001b[38;5;132;01m{\u001b[39;00muri\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and format \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mformat\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Couldn't find appropriate backend to handle uri Manav Kaul's lay 'Bali aur Shambhu' by Leher, The Dramatics Society of DCAC, Delhi University. [MZlTjI4sa68].mp3 and format None."
     ]
    }
   ],
   "source": [
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "# Initialize models for Hindi transcription\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-large\")\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-large\")\n",
    "\n",
    "def transcribe_hindi_audio(audio_path):\n",
    "    # Load and preprocess audio\n",
    "    waveform, rate = torchaudio.load(audio_path)\n",
    "    if waveform.dim() > 2:\n",
    "        waveform = waveform.mean(dim=0, keepdim=True)\n",
    "    if rate != 16000:\n",
    "        resampler = torchaudio.transforms.Resample(orig_freq=rate, new_freq=16000)\n",
    "        waveform = resampler(waveform)\n",
    "    waveform = waveform.squeeze(0)\n",
    "    \n",
    "    # Prepare input for the model\n",
    "    inputs = processor(waveform.numpy(), sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
    "    input_features = inputs.input_features\n",
    "    attention_mask = inputs.attention_mask if 'attention_mask' in inputs else None\n",
    "\n",
    "    # Generate transcription in Hindi\n",
    "    with torch.no_grad():\n",
    "        predicted_ids = model.generate(\n",
    "            input_features, \n",
    "            attention_mask=attention_mask, \n",
    "            forced_decoder_ids=processor.get_decoder_prompt_ids(language=\"hi\", task=\"transcribe\")\n",
    "        )\n",
    "    transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n",
    "    return transcription\n",
    "\n",
    "audio_path = \"Manav Kaul's lay 'Bali aur Shambhu' by Leher, The Dramatics Society of DCAC, Delhi University. [MZlTjI4sa68].mp3\"\n",
    "print(\"Transcription:\", transcribe_hindi_audio(audio_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
